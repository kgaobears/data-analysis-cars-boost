{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c343cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e51e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c918883",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Cars-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a7cf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Gender'] = data['Gender'].apply(lambda x: 0 if x == \"Female\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "570f6e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Engineer</th>\n",
       "      <th>MBA</th>\n",
       "      <th>Work Exp</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Distance</th>\n",
       "      <th>license</th>\n",
       "      <th>Opt_service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Work Exp</th>\n",
       "      <td>0.924430</td>\n",
       "      <td>0.085161</td>\n",
       "      <td>0.079911</td>\n",
       "      <td>0.039443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931810</td>\n",
       "      <td>0.395161</td>\n",
       "      <td>0.389882</td>\n",
       "      <td>0.731563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salary</th>\n",
       "      <td>0.857784</td>\n",
       "      <td>0.103673</td>\n",
       "      <td>0.079428</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.931810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475367</td>\n",
       "      <td>0.457207</td>\n",
       "      <td>0.810703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Opt_service</th>\n",
       "      <td>0.683498</td>\n",
       "      <td>0.078684</td>\n",
       "      <td>0.075514</td>\n",
       "      <td>-0.002494</td>\n",
       "      <td>0.731563</td>\n",
       "      <td>0.810703</td>\n",
       "      <td>0.548475</td>\n",
       "      <td>0.487126</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Age    Gender  Engineer       MBA  Work Exp    Salary  \\\n",
       "Work Exp     0.924430  0.085161  0.079911  0.039443  1.000000  0.931810   \n",
       "Salary       0.857784  0.103673  0.079428  0.028627  0.931810  1.000000   \n",
       "Opt_service  0.683498  0.078684  0.075514 -0.002494  0.731563  0.810703   \n",
       "\n",
       "             Distance   license  Opt_service  \n",
       "Work Exp     0.395161  0.389882     0.731563  \n",
       "Salary       0.475367  0.457207     0.810703  \n",
       "Opt_service  0.548475  0.487126     1.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data.corr()\n",
    "corr[corr['Opt_service'] >= .7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7dcce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data.drop(columns=[\"Opt_service\"])\n",
    "y = data['Opt_service']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74af6abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.912698\n",
       "1    0.087302\n",
       "Name: Opt_service, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95704d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = BaggingClassifier()\n",
    "bag.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ee50062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_examples = y_train[y_train == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7cbafd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.score(data.iloc[pos_examples.index].drop(columns=[\"Opt_service\"]), pos_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89700e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.99992"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_examples) * .95833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2862c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(estimator, X, y):\n",
    "    print(\"recall \", recall_score(y, estimator.predict(X)))\n",
    "    print(\"precision \", precision_score(y, estimator.predict(X)))\n",
    "    print(\"f1 \", f1_score(y, estimator.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f48d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d428c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(base_estimator=LogisticRegression(max_iter=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0912a9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(max_iter=2000))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d837f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d88e87dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall  1.0\n",
      "precision  1.0\n",
      "f1  1.0\n"
     ]
    }
   ],
   "source": [
    "display_scores(random_forest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "43dd69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall  1.0\n",
      "precision  0.7857142857142857\n",
      "f1  0.88\n"
     ]
    }
   ],
   "source": [
    "display_scores(bag, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca51dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9565217391304348\n",
      "0.88\n",
      "0.88\n",
      "0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,6):\n",
    "    bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=x, random_state=1))\n",
    "    bag.fit(X_train, y_train)\n",
    "    print(f1_score(y_test, bag.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "99d6e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "grad = GradientBoostingClassifier()\n",
    "xg = XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7caf218b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.fit(X_train, y_train)\n",
    "grad.fit(X_train, y_train)\n",
    "xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "915253b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall  1.0\n",
      "precision  1.0\n",
      "f1  1.0\n",
      "recall  1.0\n",
      "precision  0.9166666666666666\n",
      "f1  0.9565217391304348\n",
      "recall  1.0\n",
      "precision  1.0\n",
      "f1  1.0\n"
     ]
    }
   ],
   "source": [
    "display_scores(ada, X_test, y_test)\n",
    "display_scores(grad, X_test, y_test)\n",
    "display_scores(xg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21d0ba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.83970176e-03, 4.58604307e-04, 7.54293803e-03, 4.67118629e-08,\n",
       "       5.27197688e-03, 6.95737055e-01, 2.80311456e-01, 2.83822160e-03])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0cf3d6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MBA', 4.671186288916439e-08),\n",
       " ('Gender', 0.0004586043066376853),\n",
       " ('license', 0.0028382216046108754),\n",
       " ('Work Exp', 0.005271976876997903),\n",
       " ('Engineer', 0.0075429380331877675),\n",
       " ('Age', 0.00783970176465623),\n",
       " ('Distance', 0.28031145552263065),\n",
       " ('Salary', 0.6957370551794159)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=list(zip(X_train.columns, grad.feature_importances_))\n",
    "result.sort(key=lambda x: x[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3d075676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9787234042553191\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for x in [50, 100, 400]:\n",
    "    grad = GradientBoostingClassifier(n_estimators=x, learning_rate=0.01, random_state=1)\n",
    "    grad.fit(X_train, y_train)\n",
    "    print(f1_score(y_train, grad.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "34a93e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StackingClassifier in module sklearn.ensemble._stacking:\n",
      "\n",
      "class StackingClassifier(sklearn.base.ClassifierMixin, _BaseStacking)\n",
      " |  StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      " |  \n",
      " |  Stack of estimators with a final classifier.\n",
      " |  \n",
      " |  Stacked generalization consists in stacking the output of individual\n",
      " |  estimator and use a classifier to compute the final prediction. Stacking\n",
      " |  allows to use the strength of each individual estimator by using their\n",
      " |  output as input of a final estimator.\n",
      " |  \n",
      " |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      " |  is trained using cross-validated predictions of the base estimators using\n",
      " |  `cross_val_predict`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <stacking>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.22\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimators : list of (str, estimator)\n",
      " |      Base estimators which will be stacked together. Each element of the\n",
      " |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      " |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      " |  \n",
      " |  final_estimator : estimator, default=None\n",
      " |      A classifier which will be used to combine the base estimators.\n",
      " |      The default classifier is a\n",
      " |      :class:`~sklearn.linear_model.LogisticRegression`.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy used in\n",
      " |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      " |      cv are:\n",
      " |  \n",
      " |      * None, to use the default 5-fold cross validation,\n",
      " |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      " |      * An object to be used as a cross-validation generator,\n",
      " |      * An iterable yielding train, test splits.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and y is\n",
      " |      either binary or multiclass,\n",
      " |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      " |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. note::\n",
      " |         A larger number of split will provide no benefits if the number\n",
      " |         of training samples is large enough. Indeed, the training time\n",
      " |         will increase. ``cv`` is not used for model evaluation but for\n",
      " |         prediction.\n",
      " |  \n",
      " |  stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n",
      " |      Methods called for each base estimator. It can be:\n",
      " |  \n",
      " |      * if 'auto', it will try to invoke, for each estimator,\n",
      " |        `'predict_proba'`, `'decision_function'` or `'predict'` in that\n",
      " |        order.\n",
      " |      * otherwise, one of `'predict_proba'`, `'decision_function'` or\n",
      " |        `'predict'`. If the method is not implemented by the estimator, it\n",
      " |        will raise an error.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel all `estimators` `fit`.\n",
      " |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      " |      using all processors. See Glossary for more details.\n",
      " |  \n",
      " |  passthrough : bool, default=False\n",
      " |      When False, only the predictions of estimators will be used as\n",
      " |      training data for `final_estimator`. When True, the\n",
      " |      `final_estimator` is trained on the predictions as well as the\n",
      " |      original training data.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity level.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Class labels.\n",
      " |  \n",
      " |  estimators_ : list of estimators\n",
      " |      The elements of the estimators parameter, having been fitted on the\n",
      " |      training data. If an estimator has been set to `'drop'`, it\n",
      " |      will not appear in `estimators_`.\n",
      " |  \n",
      " |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Attribute to access any fitted sub-estimators by name.\n",
      " |  \n",
      " |  final_estimator_ : estimator\n",
      " |      The classifier which predicts given the output of `estimators_`.\n",
      " |  \n",
      " |  stack_method_ : list of str\n",
      " |      The method used by each base estimator.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  When `predict_proba` is used by each estimator (i.e. most of the time for\n",
      " |  `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n",
      " |  The first column predicted by each estimator will be dropped in the case\n",
      " |  of a binary classification problem. Indeed, both feature will be perfectly\n",
      " |  collinear.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      " |     (1992): 241-259.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.ensemble import StackingClassifier\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> estimators = [\n",
      " |  ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
      " |  ...     ('svr', make_pipeline(StandardScaler(),\n",
      " |  ...                           LinearSVC(random_state=42)))\n",
      " |  ... ]\n",
      " |  >>> clf = StackingClassifier(\n",
      " |  ...     estimators=estimators, final_estimator=LogisticRegression()\n",
      " |  ... )\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      " |  ...     X, y, stratify=y, random_state=42\n",
      " |  ... )\n",
      " |  >>> clf.fit(X_train, y_train).score(X_test, y_test)\n",
      " |  0.9...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StackingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      _BaseStacking\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict decision function for samples in X using\n",
      " |      `final_estimator_.decision_function`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      decisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          The decision function computed the final estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the estimators.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if all underlying estimators\n",
      " |          support sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  predict(self, X, **predict_params)\n",
      " |      Predict target for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      **predict_params : dict of str -> obj\n",
      " |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      " |          that this may be used to return uncertainties from some estimators\n",
      " |          with `return_std` or `return_cov`. Be aware that it will only\n",
      " |          accounts for uncertainty in the final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      " |          Predicted targets.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X using\n",
      " |      `final_estimator_.predict_proba`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      probabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n",
      " |          The class probabilities of the input samples.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Return class labels or probabilities for X for each estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n",
      " |          Prediction outputs for each estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseStacking:\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Returns the parameters given in the constructor as well as the\n",
      " |      estimators contained within the `estimators` parameter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          Setting it to True gets the various estimators and the parameters\n",
      " |          of the estimators as well.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      " |      can directly set the parameters of the estimators contained in\n",
      " |      `estimators`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : keyword arguments\n",
      " |          Specific parameters using e.g.\n",
      " |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      " |          parameters of the estimator, the individual estimator of the\n",
      " |          estimators can also be set, or can be removed by setting them to\n",
      " |          'drop'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  named_estimators\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      " |  \n",
      " |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StackingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b72e003f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = AdaBoostClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "bag = GradientBoostingClassifier()\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6607efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(estimators=[('tree', dtree), ('bag',bag)], final_estimator=XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5465b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:45:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('tree', AdaBoostClassifier()),\n",
       "                               ('bag', GradientBoostingClassifier())],\n",
       "                   final_estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                                 colsample_bylevel=None,\n",
       "                                                 colsample_bynode=None,\n",
       "                                                 colsample_bytree=None,\n",
       "                                                 enable_categorical=False,\n",
       "                                                 gamma=None, gpu_id=None,\n",
       "                                                 importance_type=None,\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=None,\n",
       "                                                 min_child_weight=None,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=100, n_jobs=None,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 predictor=None,\n",
       "                                                 random_state=None,\n",
       "                                                 reg_alpha=None,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=None,\n",
       "                                                 tree_method=None,\n",
       "                                                 validate_parameters=None,\n",
       "                                                 verbosity=None))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2dad8c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall  1.0\n",
      "precision  0.7857142857142857\n",
      "f1  0.88\n"
     ]
    }
   ],
   "source": [
    "display_scores(stack, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e3d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
